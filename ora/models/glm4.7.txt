python----




from openai import OpenAI
import os
import sys

_USE_COLOR = sys.stdout.isatty() and os.getenv("NO_COLOR") is None
_REASONING_COLOR = "\033[90m" if _USE_COLOR else ""
_RESET_COLOR = "\033[0m" if _USE_COLOR else ""

client = OpenAI(
  base_url = "https://integrate.api.nvidia.com/v1",
  api_key = "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"
)


completion = client.chat.completions.create(
  model="z-ai/glm4.7",
  messages=[{"role":"user","content":""}],
  temperature=1,
  top_p=1,
  max_tokens=16384,
  extra_body={"chat_template_kwargs":{"enable_thinking":True,"clear_thinking":False}},
  stream=True
)

for chunk in completion:
  if not getattr(chunk, "choices", None):
    continue
  if len(chunk.choices) == 0 or getattr(chunk.choices[0], "delta", None) is None:
    continue
  delta = chunk.choices[0].delta
  reasoning = getattr(delta, "reasoning_content", None)
  if reasoning:
    print(f"{_REASONING_COLOR}{reasoning}{_RESET_COLOR}", end="")
  if getattr(delta, "content", None) is not None:
    print(delta.content, end="")
  


  



LANG-------


from langchain_nvidia_ai_endpoints import ChatNVIDIA


client = ChatNVIDIA(
  model="z-ai/glm4.7",
  api_key="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", 
  temperature=1,
  top_p=1,
  max_tokens=16384,
  extra_body={"chat_template_kwargs":{"enable_thinking":True,"clear_thinking":False}},
)

for chunk in client.stream([{"role":"user","content":""}]):
  
    if chunk.additional_kwargs and "reasoning_content" in chunk.additional_kwargs:
      print(chunk.additional_kwargs["reasoning_content"], end="")
  
    print(chunk.content, end="")









NODE-------



import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: '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',
  baseURL: 'https://integrate.api.nvidia.com/v1',
})
 
async function main() {
  const completion = await openai.chat.completions.create({
    model: "z-ai/glm4.7",
    messages: [{"role":"user","content":""}],
    temperature: 1,
    top_p: 1,
    max_tokens: 16384,
    chat_template_kwargs: {"enable_thinking":true,"clear_thinking":false},
    stream: true
  })
   
  for await (const chunk of completion) {
        const reasoning = chunk.choices[0]?.delta?.reasoning_content;
    if (reasoning) process.stdout.write(reasoning);
        process.stdout.write(chunk.choices[0]?.delta?.content || '')
    
  }
  
}

main();




SHELL----


invoke_url='https://integrate.api.nvidia.com/v1/chat/completions'

authorization_header='Authorization: Bearer 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'
accept_header='Accept: application/json'
content_type_header='Content-Type: application/json'

data=$'{
  "model": "z-ai/glm4.7",
  "messages": [
    {
      "role": "user",
      "content": ""
    }
  ],
  "temperature": 1,
  "top_p": 1,
  "max_tokens": 16384,
  "seed": 42,
  "stream": true,
  "chat_template_kwargs": {
    "enable_thinking": true,
    "clear_thinking": false
  }
}'

response=$(curl --silent -i -w "\n%{http_code}" --request POST \
  --url "$invoke_url" \
  --header "$authorization_header" \
  --header "$accept_header" \
  --header "$content_type_header" \
  --data "$data"
)

echo "$response"




GLM-4.7
Description
GLM-4.7 is a large language model developed by Z.ai (formerly THUDM/Zhipu AI) optimized for coding, reasoning, and tool use. It features significant improvements in multilingual agentic coding, terminal-based tasks, UI generation, and complex mathematical reasoning compared to its predecessor GLM-4.6. The model introduces Interleaved Thinking, Preserved Thinking, and Turn-level Thinking capabilities for more stable and controllable complex task execution.

This model is ready for commercial/non-commercial use.

Third-Party Community Consideration:
This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIA GLM-4.7 Model Card

License and Terms of Use:
GOVERNING TERMS: The trial service is governed by the NVIDIA API Trial Terms of Service. Use of this model is governed by the NVIDIA Open Model License. Additional Information: MIT.

Deployment Geography:
Global

Use Case:
Use Case: Developers and researchers can use GLM-4.7 for coding assistance, agentic workflows, terminal-based automation, mathematical reasoning, and general text generation tasks including chat, creative writing, and role-play scenarios.

Release Date:
Build.NVIDIA.com: 01/2026 via link

Huggingface: 12/22/2025 via link

Reference(s):
References:

GLM-4.7 Technical Blog
GLM-4.5 Technical Report (arXiv)
Z.ai API Platform
GitHub Repository
Model Architecture:
Architecture Type: Transformer

Network Architecture: GLM (General Language Model)

Total Parameters: 358B

Base Model: GLM-4.5/GLM-4.6

Input:
Input Types: Text

Input Formats: String

Input Parameters: One Dimensional (1D)

Other Input Properties: Supports multi-turn conversations, tool calling, and system prompts.

Input Context Length (ISL): 131,072 tokens

Output:
Output Types: Text

Output Format: String

Output Parameters: One Dimensional (1D)

Other Output Properties: Supports streaming, structured output, and reasoning traces.

Output Context Length (OSL): 131,072 tokens

Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.

Software Integration:
Runtime Engines:

vLLM: nightly
SGLang: dev
Transformers: 4.57.3+
Supported Hardware:

NVIDIA Ampere: A100
NVIDIA Hopper: H100
Operating Systems: Linux

The integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment.

Model Version(s)
GLM-4 v4.7

Training, Testing, and Evaluation Datasets:
Training Dataset
Data Modality: Text (English, Chinese)

Training Data Collection: Undisclosed

Training Labeling: Undisclosed

Training Properties: Undisclosed

Testing Dataset
Testing Data Collection: Undisclosed

Testing Labeling: Undisclosed

Testing Properties: Undisclosed

Evaluation Dataset
Evaluation Data Collection: Automated

Evaluation Labeling: Hybrid: Human, Automated

Evaluation Properties: Benchmark testing conditions: Multi-domain evaluation including reasoning (8 benchmarks), coding (5 benchmarks), and agent tasks (3+ benchmarks). Standard industry benchmarks with comparable methodology across models.

Evaluation Benchmark Score: GLM-4.7 demonstrates strong performance across 17 benchmarks spanning reasoning (8), coding (5), and agent tasks (3+). Key highlights: AIME 2025 (95.7%), HMMT Feb. 2025 (97.1%), GPQA-Diamond (85.7%), LiveCodeBench-v6 (84.9%), tau2-Bench (87.4%).

Detailed Benchmark Comparison Table
Inference
Acceleration Engine: SGLang

Test Hardware: NVIDIA H100x8

Additional Details
Key Features
Interleaved Thinking: The model thinks before every response and tool calling, improving instruction following and generation quality.
Preserved Thinking: In coding agent scenarios, the model retains thinking blocks across multi-turn conversations, reducing information loss.
Turn-level Thinking: Per-turn control over reasoning - disable for lightweight requests, enable for complex tasks.
Recommended Inference Settings
Task Type	Temperature	Top-p	Max Tokens
Default	1.0	0.95	131,072
SWE-bench/Terminal	0.7	1.0	16,384
τ^2-Bench	0	-	16,384
For τ^2-Bench evaluation, zai-org added an additional prompt to the Retail and Telecom user interaction to avoid failure modes caused by users ending the interaction incorrectly. For the Airline domain, we applied the domain fixes as proposed in the Claude Opus 4.5 release report.

Deployment Examples
Serve GLM-4.7 Locally: For local deployment, GLM-4.7 supports inference frameworks including vLLM and SGLang. Comprehensive deployment instructions are available in the official Github repository.

vLLM and SGLang only support GLM-4.7 on their main branches. you can use their official docker images for inference.

vLLM:

Bash

Copy
vllm serve zai-org/GLM-4.7-FP8 \
     --tensor-parallel-size 4 \
     --tool-call-parser glm47 \
     --reasoning-parser glm45 \
     --enable-auto-tool-choice
SGLang:

Bash

Copy
python3 -m sglang.launch_server \
  --model-path zai-org/GLM-4.7-FP8 \
  --tp-size 8 \
  --tool-call-parser glm47 \
  --reasoning-parser glm45
transformers: using with transformers as 4.57.3 and then run:

Python

Copy
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

MODEL_PATH = "zai-org/GLM-4.7"
messages = [{"role": "user", "content": "hello"}]
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt",
)
model = AutoModelForCausalLM.from_pretrained(
    pretrained_model_name_or_path=MODEL_PATH,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
inputs = inputs.to(model.device)
generated_ids = model.generate(**inputs, max_new_tokens=128, do_sample=False)
output_text = tokenizer.decode(generated_ids[0][inputs.input_ids.shape[1] :])
print(output_text)
Parameter Instructions
For agentic tasks of GLM-4.7, please turn on Preserved Thinking mode by adding the following config (only sglang support):

Bash

Copy
  "chat_template_kwargs": {
      "enable_thinking": true,
      "clear_thinking": false
  }
When using vLLM and SGLang, thinking mode is enabled by default when sending requests. If you want to disable the thinking switch, you need to add the "chat_template_kwargs": {"enable_thinking": False} parameter.

Both support tool calling. Please use OpenAI-style tool description format for calls.


